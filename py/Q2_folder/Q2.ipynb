{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the Q1 notebook, we need to load the data in a useable form (i.e. a pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to where the csv file is stored on your pc\n",
    "path = '/Users/ryanu/Documents/Uni/ACT/SDSS-DR14-Classification/SDSS Data.csv'\n",
    "data = pd.read_csv(path)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start off using the same features as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['u', 'g', 'r', 'i', 'z']]\n",
    "labels = data['class']\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Q1 with the decision tree, we need to split the data between training and testing. However, now we're going also split out a validation set. Where the training set is used to train the model, and the testing set is used to measure the models performance, the validation set will be used to tune hyperparameters (e.g., learning rate, architecture) and monitor overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, verification, and testing sets\n",
    "    # train_test_split() splits the data into training and testing sets\n",
    "    # test_size=0.2 specifies that 20% of the data should be used for testing\n",
    "    # random_state=42 is a random seed used to shuffle the data\n",
    "    # The data is split into training and validation sets in a 80:20 ratio\n",
    "    # The training set is then split into training and validation sets in a 80:20 ratio\n",
    "    # The final data is split into training, validation, and testing sets in a 64:16:20 ratio\n",
    "features_train_val, features_test, label_train_val, label_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "features_train, features_val, label_train, label_val = train_test_split(features_train_val, label_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network we need to get the data into the right format to work with. The first step is to normalise all the data. Neural networks often perform better with normalised data because they are sensitive to the scale of the input features Standardization ensures that features with larger ranges donâ€™t dominate, and makes the model converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliase the StandardScaler() function\n",
    "    # It's important to initialise the StandardScaler() function, then use it for all the data sets to ensure that the same scaling is applied to all the data sets\n",
    "    # The StandardScaler() function scales the data so that it has a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler() function to the training data\n",
    "    # The fit_transform() function fits the StandardScaler() function to the training data and then scales the training data\n",
    "    # The transform() function scales the validation and testing data using the same scaling as the training data\n",
    "    # This ensures that the validation and testing data are scaled in the same way as the training data\n",
    "features_train_normalised = scaler.fit_transform(features_train)\n",
    "features_val_normalised = scaler.transform(features_val)\n",
    "features_test_normalised = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to convert the label names (Star, Galaxy, QSO) into numbers as neural networks expect numerical inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels using the LabelEncoder() function\n",
    "    # Again, it's important to initialise the LabelEncoder() function, then use it for all the data sets to ensure that the same encoding is applied to all the data sets\n",
    "    # The LabelEncoder() function encodes the labels, in alphabetical order, as integers starting from 0 (e.g. Galaxy is 0, QSO is 1, Star is 2)\n",
    "    # This is necessary because the labels need to be integers for the model to be able to use them\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder() function to the training labels\n",
    "    # The fit_transform() function fits the LabelEncoder() function to the training labels and then encodes the training labels\n",
    "    # The transform() function encodes the validation and testing labels using the same encoding as the training labels\n",
    "    # This ensures that the validation and testing labels are encoded in the same way as the training labels\n",
    "label_train_encoded = label_encoder.fit_transform(label_train)\n",
    "label_val_encoded = label_encoder.transform(label_val)\n",
    "label_test_encoded = label_encoder.transform(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to store the datasets as a PyTorch tensor, which are similar to NumPy arrays but have some unique features that make them more suitable for machine learning tasks.\n",
    "\n",
    "- Multi-Dimensional Arrays:\n",
    "    - Tensors can have any number of dimensions, making them versatile for representing various types of data, such as scalars (0D), vectors (1D), matrices (2D), and higher-dimensional arrays\n",
    "- GPU Acceleration:\n",
    "    - PyTorch tensors can be moved to and operated on using GPUs, which significantly speeds up computations, especially for large-scale machine learning models\n",
    "- Automatic Differentiation:\n",
    "    - PyTorch tensors support automatic differentiation, which is essential for training neural networks. This feature is provided by PyTorch's autograd module, which automatically computes gradients for tensor operations\n",
    "- Interoperability with NumPy:\n",
    "    - PyTorch tensors can be easily converted to and from NumPy arrays, allowing seamless integration with existing NumPy-based code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and labels into PyTorch tensors\n",
    "    # torch.tensor() creates a tensor from a NumPy array\n",
    "    # dtype=torch.float32 and dtype=torch.long specify the data type of the tensor\n",
    "features_train_tensor = torch.tensor(features_train_normalised, dtype=torch.float32)\n",
    "features_val_tensor = torch.tensor(features_val_normalised, dtype=torch.float32)\n",
    "features_test_tensor = torch.tensor(features_test_normalised, dtype=torch.float32)\n",
    "label_train_tensor = torch.tensor(label_train_encoded, dtype=torch.long)\n",
    "label_val_tensor = torch.tensor(label_val_encoded, dtype=torch.long)\n",
    "label_test_tensor = torch.tensor(label_test_encoded, dtype=torch.long)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

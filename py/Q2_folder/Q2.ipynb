{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the Q1 notebook, we need to load the data in a useable form (i.e. a pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to where the csv file is stored on your pc\n",
    "path = '/Users/ryanu/Documents/Uni/ACT/SDSS-DR14-Classification/SDSS Data.csv'\n",
    "data = pd.read_csv(path)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start off using the same features as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['u', 'g', 'r', 'i', 'z']]\n",
    "labels = data['class']\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Q1 with the decision tree, we need to split the data between training and testing. However, now we're going also split out a validation set. Where the training set is used to train the model, and the testing set is used to measure the models performance, the validation set will be used to tune hyperparameters (e.g., learning rate, architecture) and monitor overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, verification, and testing sets\n",
    "    # train_test_split() splits the data into training and testing sets\n",
    "    # test_size=0.2 specifies that 20% of the data should be used for testing\n",
    "    # random_state=42 is a random seed used to shuffle the data\n",
    "    # The data is split into training and validation sets in a 80:20 ratio\n",
    "    # The training set is then split into training and validation sets in a 80:20 ratio\n",
    "    # The final data is split into training, validation, and testing sets in a 64:16:20 ratio\n",
    "features_train_val, features_test, label_train_val, label_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "features_train, features_val, label_train, label_val = train_test_split(features_train_val, label_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network we need to get the data into the right format to work with. The first step is to normalise all the data. Neural networks often perform better with normalised data because they are sensitive to the scale of the input features Standardization ensures that features with larger ranges donâ€™t dominate, and makes the model converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliase the StandardScaler() function\n",
    "    # It's important to initialise the StandardScaler() function, then use it for all the data sets to ensure that the same scaling is applied to all the data sets\n",
    "    # The StandardScaler() function scales the data so that it has a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler() function to the training data\n",
    "    # The fit_transform() function fits the StandardScaler() function to the training data and then scales the training data\n",
    "    # The transform() function scales the validation and testing data using the same scaling as the training data\n",
    "    # This ensures that the validation and testing data are scaled in the same way as the training data\n",
    "features_train_normalised = scaler.fit_transform(features_train)\n",
    "features_val_normalised = scaler.transform(features_val)\n",
    "features_test_normalised = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to convert the label names (Star, Galaxy, QSO) into numbers as neural networks expect numerical inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels using the LabelEncoder() function\n",
    "    # Again, it's important to initialise the LabelEncoder() function, then use it for all the data sets to ensure that the same encoding is applied to all the data sets\n",
    "    # The LabelEncoder() function encodes the labels, in alphabetical order, as integers starting from 0 (e.g. Galaxy is 0, QSO is 1, Star is 2)\n",
    "    # This is necessary because the labels need to be integers for the model to be able to use them\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder() function to the training labels\n",
    "    # The fit_transform() function fits the LabelEncoder() function to the training labels and then encodes the training labels\n",
    "    # The transform() function encodes the validation and testing labels using the same encoding as the training labels\n",
    "    # This ensures that the validation and testing labels are encoded in the same way as the training labels\n",
    "label_train_encoded = label_encoder.fit_transform(label_train)\n",
    "label_val_encoded = label_encoder.transform(label_val)\n",
    "label_test_encoded = label_encoder.transform(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to store the datasets as a PyTorch tensor, which are similar to NumPy arrays but have some unique features that make them more suitable for machine learning tasks.\n",
    "\n",
    "- Multi-Dimensional Arrays:\n",
    "    - Tensors can have any number of dimensions, making them versatile for representing various types of data, such as scalars (0D), vectors (1D), matrices (2D), and higher-dimensional arrays\n",
    "- GPU Acceleration:\n",
    "    - PyTorch tensors can be moved to and operated on using GPUs, which significantly speeds up computations, especially for large-scale machine learning models\n",
    "- Automatic Differentiation:\n",
    "    - PyTorch tensors support automatic differentiation, which is essential for training neural networks. This feature is provided by PyTorch's autograd module, which automatically computes gradients for tensor operations\n",
    "- Interoperability with NumPy:\n",
    "    - PyTorch tensors can be easily converted to and from NumPy arrays, allowing seamless integration with existing NumPy-based code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and labels into PyTorch tensors\n",
    "    # torch.tensor() creates a tensor from a NumPy array\n",
    "    # dtype=torch.float32 and dtype=torch.long specify the data type of the tensor\n",
    "features_train_tensor = torch.tensor(features_train_normalised, dtype=torch.float32)\n",
    "features_val_tensor = torch.tensor(features_val_normalised, dtype=torch.float32)\n",
    "features_test_tensor = torch.tensor(features_test_normalised, dtype=torch.float32)\n",
    "label_train_tensor = torch.tensor(label_train_encoded, dtype=torch.long)\n",
    "label_val_tensor = torch.tensor(label_val_encoded, dtype=torch.long)\n",
    "label_test_tensor = torch.tensor(label_test_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data ready, we can define the actual neural network (NN). The SimpleNN class defines a Feedforward Neural Network (FNN), which is the simplest type of NN. It's called feedforward because the data flows in one direction; from the input layer, through any hidden layers, into the output layer. There are no loops or cycles in the network.\n",
    "\n",
    "The first thing the SimpleNN class does it inherit from PyTorch's nn.Module class, which is the base class for all neural network models in PyTorch. This inheritance provides the necessary structure and methods to define and train a neural network.\n",
    "\n",
    "It then sets up the layers required by the NN.\n",
    "\n",
    "- The Input Layer:\n",
    "    - The input layer is the first layer of the NN and has the same number of neurons as the number of input features\n",
    "    - It isn't explicitly defined in the SimpleNN class because it's just the input data\n",
    "- First Fully Connected Layer:\n",
    "    - The first fully connected layer is the first hidden layer of the NN\n",
    "    - It is defined by the nn.Linear() class, which creates a fully connected layer\n",
    "    - The number of input features is specified by the input_size parameter\n",
    "    - The number of neurons in the hidden layer is specified by the hidden_size parameter\n",
    "- Rectified Linear Unit (ReLU) Activation Function:\n",
    "    - The ReLU activation function is applied to the output of the first fully connected layer\n",
    "    - It is defined by the nn.ReLU() class and is used to introduce non-linearity into the NN\n",
    "- Second Fully Connected Layer:\n",
    "    - The second fully connected layer is the output layer of the NN\n",
    "    - It is defined by the nn.Linear() class\n",
    "    - The number of neurons in the output layer is specified by the output_size parameter\n",
    "- Log Softmax Activation Function:\n",
    "    - The Log Softmax activation function is applied to the output of the second fully connected layer\n",
    "    - It is defined by the nn.LogSoftmax() class and is used to convert the output into log probabilities\n",
    "\n",
    "Next it defines the forward() method, which specifies how the data flows through the NN. The forward() method takes the input data as a tensor, then passes it to the first connected layer. The linear output is then passed through the ReLU activation function to help the NN learn complex relationship in the data. The tensor is then passed into the second connected layer, and the softmax activation function where log probabilities are calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDSSClassifier(nn.Module):\n",
    "    '''\n",
    "    This class defines the neural network model for the classification task. The neural network model consists of three fully connected \n",
    "        layers with ReLU activation functions and a softmax activation function at the output layer. The neural network model is defined in\n",
    "        the __init__() function and the forward pass is defined in the forward() function.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        '''\n",
    "        This function initialises the SDSSClassifier class\n",
    "\n",
    "        :param input_size: The number of input features, e.g. 5 for [u, g, r, i, z]\n",
    "        :param hidden_size: The number of neurons in the hidden layer\n",
    "        :param num_classes: The number of output classes, e.g. 3 for [Galaxy, QSO, and Star]\n",
    "        '''\n",
    "        # The super() function is used to call the __init__() function of the parent class (nn.Module)\n",
    "        super(SDSSClassifier, self).__init__()\n",
    "\n",
    "        # Define the layers of the neural network\n",
    "            # nn.Linear() defines a fully connected layer\n",
    "                # The first argument is the number of input neurons\n",
    "                # The second argument is the number of output neurons\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # First fully connected layer\n",
    "        self.relu = nn.ReLU() # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes) # Third fully connected layer\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax activation function\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        '''\n",
    "        This function defines the forward pass of the neural network model and is called when the neural network model is run. The forward\n",
    "            pass is the process of inputting the input features into the neural network and obtaining an output.\n",
    "\n",
    "        :param input_features: The input features\n",
    "        :return: The output of the neural network (the class probabilities)\n",
    "        '''\n",
    "        input_features = self.fc1(input_features) # Pass the input features through the first fully connected layer\n",
    "        input_features = self.relu(input_features) # Pass the output of the first fully connected layer through the ReLU activation function\n",
    "        input_features = self.fc2(input_features) # Pass the output of the ReLU activation function through the second fully connected layer\n",
    "        input_features = self.relu(input_features) # Pass the output of the second fully connected layer through the ReLU activation function\n",
    "        input_features = self.fc3(input_features) # Pass the output of the ReLU activation function through the third fully connected layer\n",
    "        return self.softmax(input_features) # Apply the softmax activation function to the output of the third fully connected layer and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the NN has been defined, we can start to initialise it. The input, hidden, and output sizes are defined and then fed into the SimpleNN class to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model, loss function, and optimiser\n",
    "# Input size is the number of input features (e.g., 5 for u, g, r, i, z) we will use\n",
    "input_size = features_train_tensor.shape[1]\n",
    "\n",
    "# Hidden size is the number of neurons in the hidden layer\n",
    "hidden_size = 64  # You can change this value to see how it affects the performance of the model\n",
    "\n",
    "# Output size is the number of classes (e.g., 3 for star, galaxy, quasar)\n",
    "    # The number of classes is the number of unique labels in the training data\n",
    "    # The np.unique() function returns the unique elements in an array, in this case the unique labels in the training data\n",
    "output_size = len(np.unique(label_train_tensor))\n",
    "\n",
    "# Create an instance of the SimpleNN class\n",
    "model = SimpleNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the loss function and optimiser.\n",
    "\n",
    "- Loss function:\n",
    "    - Used to calculate the error between the predicted output of the neural network and the actual labels\n",
    "    - A hyperparameter that needs to be tuned to achieve the best performance of the model\n",
    "\n",
    "- Optmiser:\n",
    "    - Used to update the weights of the neural network based on the error calculated by the loss function\n",
    "    - There are many different optimisers available in PyTorch, such as Adam, SGD, RMSprop, etc. We use Adam in this example because it is a popular choice for many tasks, and commonly used for NNs\n",
    "    - The Adam optimiser is an adaptive learning rate optimiser that adjusts the learning rate during training, which can help the model converge faster and achieve better performance\n",
    "\n",
    "The loss function and optimiser are defined outside the neural network model class because they are not part of the neural network architecture but are instead used to train the neural network. This also allows them to be easily changed or modified without affecting the neural network architecture.\n",
    "\n",
    "- Learning rate:\n",
    "    - The learning rate controls how much the weights of the neural network are updated during training\n",
    "    - A higher learning rate means the weights are updated more and a smaller learning rate means the weights are updated less, during the training.\n",
    "    - If the learning rate is too high, the model may converge too quickly or diverge, as it overshoots the minimum of the loss function\n",
    "    - If the learning rate is too low, the training process can get stuck in a local minimum and/or take a very long time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimiser\n",
    "# nn.CrossEntropyLoss() is the loss function used for classification tasks with multiple classes\n",
    "# optim.Adam() is the optimiser used to update the weights of the neural network\n",
    "\n",
    "# Define the loss function\n",
    "    # nn.CrossEntropyLoss() is the loss function used for classification tasks with multiple classes\n",
    "    # The CrossEntropyLoss() function combines the softmax activation function and the negative log likelihood loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimiser\n",
    "    # optim.Adam() is the optimiser used to update the weights of the neural network\n",
    "    # The Adam optimiser is an extension of the stochastic gradient descent optimiser\n",
    "    # The Adam optimiser adapts the learning rate for each parameter during training\n",
    "    # The learning rate is specified by the lr argument\n",
    "    # The model.parameters() function specifies the parameters that need to be updated by the optimiser, which in this case are the weights of the neural network\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

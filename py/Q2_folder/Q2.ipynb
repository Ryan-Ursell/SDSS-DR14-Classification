{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn, optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in the Q1 notebook, we need to load the data in a useable form (i.e. a pandas dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the path to where the csv file is stored on your pc\n",
    "path = '/Users/ryanu/Documents/Uni/ACT/SDSS-DR14-Classification/SDSS Data.csv'\n",
    "data = pd.read_csv(path)\n",
    "#data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to start off using the same features as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['u', 'g', 'r', 'i', 'z']]\n",
    "labels = data['class']\n",
    "#features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like in Q1 with the decision tree, we need to split the data between training and testing. However, now we're going also split out a validation set. Where the training set is used to train the model, and the testing set is used to measure the models performance, the validation set will be used to tune hyperparameters (e.g., learning rate, architecture) and monitor overfitting during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, verification, and testing sets\n",
    "    # train_test_split() splits the data into training and testing sets\n",
    "    # test_size=0.2 specifies that 20% of the data should be used for testing\n",
    "    # random_state=42 is a random seed used to shuffle the data\n",
    "    # The data is split into training and validation sets in a 80:20 ratio\n",
    "    # The training set is then split into training and validation sets in a 80:20 ratio\n",
    "    # The final data is split into training, validation, and testing sets in a 64:16:20 ratio\n",
    "features_train_val, features_test, label_train_val, label_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "features_train, features_val, label_train, label_val = train_test_split(features_train_val, label_train_val, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a neural network we need to get the data into the right format to work with. The first step is to normalise all the data. Neural networks often perform better with normalised data because they are sensitive to the scale of the input features Standardization ensures that features with larger ranges donâ€™t dominate, and makes the model converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiliase the StandardScaler() function\n",
    "    # It's important to initialise the StandardScaler() function, then use it for all the data sets to ensure that the same scaling is applied to all the data sets\n",
    "    # The StandardScaler() function scales the data so that it has a mean of 0 and a standard deviation of 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler() function to the training data\n",
    "    # The fit_transform() function fits the StandardScaler() function to the training data and then scales the training data\n",
    "    # The transform() function scales the validation and testing data using the same scaling as the training data\n",
    "    # This ensures that the validation and testing data are scaled in the same way as the training data\n",
    "features_train_normalised = scaler.fit_transform(features_train)\n",
    "features_val_normalised = scaler.transform(features_val)\n",
    "features_test_normalised = scaler.transform(features_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to convert the label names (Star, Galaxy, QSO) into numbers as neural networks expect numerical inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the labels using the LabelEncoder() function\n",
    "    # Again, it's important to initialise the LabelEncoder() function, then use it for all the data sets to ensure that the same encoding is applied to all the data sets\n",
    "    # The LabelEncoder() function encodes the labels, in alphabetical order, as integers starting from 0 (e.g. Galaxy is 0, QSO is 1, Star is 2)\n",
    "    # This is necessary because the labels need to be integers for the model to be able to use them\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the LabelEncoder() function to the training labels\n",
    "    # The fit_transform() function fits the LabelEncoder() function to the training labels and then encodes the training labels\n",
    "    # The transform() function encodes the validation and testing labels using the same encoding as the training labels\n",
    "    # This ensures that the validation and testing labels are encoded in the same way as the training labels\n",
    "label_train_encoded = label_encoder.fit_transform(label_train)\n",
    "label_val_encoded = label_encoder.transform(label_val)\n",
    "label_test_encoded = label_encoder.transform(label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then want to store the datasets as a PyTorch tensor, which are similar to NumPy arrays but have some unique features that make them more suitable for machine learning tasks.\n",
    "\n",
    "- Multi-Dimensional Arrays:\n",
    "    - Tensors can have any number of dimensions, making them versatile for representing various types of data, such as scalars (0D), vectors (1D), matrices (2D), and higher-dimensional arrays\n",
    "- GPU Acceleration:\n",
    "    - PyTorch tensors can be moved to and operated on using GPUs, which significantly speeds up computations, especially for large-scale machine learning models\n",
    "- Automatic Differentiation:\n",
    "    - PyTorch tensors support automatic differentiation, which is essential for training neural networks. This feature is provided by PyTorch's autograd module, which automatically computes gradients for tensor operations\n",
    "- Interoperability with NumPy:\n",
    "    - PyTorch tensors can be easily converted to and from NumPy arrays, allowing seamless integration with existing NumPy-based code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert features and labels into PyTorch tensors\n",
    "    # torch.tensor() creates a tensor from a NumPy array\n",
    "    # dtype=torch.float32 and dtype=torch.long specify the data type of the tensor\n",
    "features_train_tensor = torch.tensor(features_train_normalised, dtype=torch.float32)\n",
    "features_val_tensor = torch.tensor(features_val_normalised, dtype=torch.float32)\n",
    "features_test_tensor = torch.tensor(features_test_normalised, dtype=torch.float32)\n",
    "label_train_tensor = torch.tensor(label_train_encoded, dtype=torch.long)\n",
    "label_val_tensor = torch.tensor(label_val_encoded, dtype=torch.long)\n",
    "label_test_tensor = torch.tensor(label_test_encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final preprocessing we need to complete is to create dataloaders for the train, validation, and test sets.\n",
    "\n",
    "- Dataset:\n",
    "    - We wrap both the feature and label tensor for the training, validation, and testing sets together into a Dataset object\n",
    "    - Treating the tensors as a dataset likes this allows us to make a Dataloader\n",
    "\n",
    "- Dataloader:\n",
    "    - Dataloaders provide an efficient way to iterate over a dataset\n",
    "    - They are designed to handle batching, shuffling, and parallel data loading, making it easier to feed data into a model during training and evaluation\n",
    "    - Batching divindes the dataset into small subsets that are processed sequentially, which is more efficient than processing them one at a time\n",
    "    - Shuffling the data introduces randomness and improves the generalisation of the model by ensure the model doesn't just learn the order of the data\n",
    "    - Parallel data loading speeds the data loading process up by running multiple worker processes at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataset for the training, validation, and testing sets\n",
    "    # torch.utils.data.TensorDataset() creates a dataset from the tensors\n",
    "    # A dataset is a collection of features and labels\n",
    "    # The dataset is used to create a DataLoader\n",
    "train_dataset = torch.utils.data.TensorDataset(features_train_tensor, label_train_tensor)\n",
    "val_dataset = torch.utils.data.TensorDataset(features_val_tensor, label_val_tensor)\n",
    "test_dataset = torch.utils.data.TensorDataset(features_test_tensor, label_test_tensor)\n",
    "\n",
    "# Create a DataLoader for the training, validation, and testing sets\n",
    "    # torch.utils.data.DataLoader() creates a DataLoader from the dataset\n",
    "    # A DataLoader is an iterable that provides batches of data\n",
    "    # batch_size=32 specifies that each batch should contain 32 samples, can be finetuned\n",
    "    # shuffle specifies whether the data should be shuffled or not\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data ready, we can define the actual neural network (NN). The SimpleNN class defines a Feedforward Neural Network (FNN), which is the simplest type of NN. It's called feedforward because the data flows in one direction; from the input layer, through any hidden layers, into the output layer. There are no loops or cycles in the network.\n",
    "\n",
    "The first thing the SimpleNN class does it inherit from PyTorch's nn.Module class, which is the base class for all neural network models in PyTorch. This inheritance provides the necessary structure and methods to define and train a neural network.\n",
    "\n",
    "It then sets up the layers required by the NN.\n",
    "\n",
    "- The Input Layer:\n",
    "    - The input layer is the first layer of the NN and has the same number of neurons as the number of input features\n",
    "    - It isn't explicitly defined in the SimpleNN class because it's just the input data\n",
    "- First Fully Connected Layer:\n",
    "    - The first hiddem layer of the NN\n",
    "    - Tranforms the input data and outputs it to the ReLu activation function\n",
    "    = Transformation is linear: output = input * weight matrix * bias vector\n",
    "        - Weight matrix and bias vector are learned during the training\n",
    "- Rectified Linear Unit (ReLU) Activation Function:\n",
    "    - The ReLU activation function is applied to the output of the first fully connected layer\n",
    "    - Introduces non-linearity into the NN by setting any negative inputs to zero\n",
    "- Second Fully Connected Layer:\n",
    "    - Takes the output from the ReLU function as an input and performs another linear transformation\n",
    "- Third Fully Connected Layer:\n",
    "    - Takes the output from the second layer and outputs to the Log Softmax activation function\n",
    "- Log Softmax Activation Function:\n",
    "    - Ensures all inputs are between 0 and 1, and that they sum to 1\n",
    "    - This ensentially turns computes probabilities\n",
    "\n",
    "Next it defines the forward() method, which specifies how the data flows through the NN. The forward() method takes the input data as a tensor, then passes it to the first connected layer. The linear output is then passed through the ReLU activation function to help the NN learn complex relationship in the data. The tensor is then passed into the second connected layer, the third, and finally the softmax activation function where log probabilities are calculated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDSSClassifier(nn.Module):\n",
    "    '''\n",
    "    This class defines the neural network model for the classification task. The neural network model consists of three fully connected \n",
    "        layers with ReLU activation functions and a softmax activation function at the output layer. The neural network model is defined in\n",
    "        the __init__() function and the forward pass is defined in the forward() function.\n",
    "    '''\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        '''\n",
    "        This function initialises the SDSSClassifier class\n",
    "\n",
    "        :param input_size: The number of input features, e.g. 5 for [u, g, r, i, z]\n",
    "        :param hidden_size: The number of neurons in the hidden layer\n",
    "        :param num_classes: The number of output classes, e.g. 3 for [Galaxy, QSO, and Star]\n",
    "        '''\n",
    "        # The super() function is used to call the __init__() function of the parent class (nn.Module)\n",
    "        super(SDSSClassifier, self).__init__()\n",
    "\n",
    "        # Define the layers of the neural network\n",
    "            # nn.Linear() defines a fully connected layer\n",
    "                # The first argument is the number of input neurons\n",
    "                # The second argument is the number of output neurons\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # First fully connected layer\n",
    "        self.relu = nn.ReLU() # ReLU activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size) # Second fully connected layer\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes) # Third fully connected layer\n",
    "        self.softmax = nn.Softmax(dim=1) # Softmax activation function\n",
    "\n",
    "    def forward(self, input_features):\n",
    "        '''\n",
    "        This function defines the forward pass of the neural network model and is called when the neural network model is run. The forward\n",
    "            pass is the process of inputting the input features into the neural network and obtaining an output.\n",
    "\n",
    "        :param input_features: The input features\n",
    "        :return: The output of the neural network (the class probabilities)\n",
    "        '''\n",
    "        input_features = self.fc1(input_features) # Pass the input features through the first fully connected layer\n",
    "        input_features = self.relu(input_features) # Pass the output of the first fully connected layer through the ReLU activation function\n",
    "        input_features = self.fc2(input_features) # Pass the output of the ReLU activation function through the second fully connected layer\n",
    "        input_features = self.relu(input_features) # Pass the output of the second fully connected layer through the ReLU activation function\n",
    "        input_features = self.fc3(input_features) # Pass the output of the ReLU activation function through the third fully connected layer\n",
    "        return self.softmax(input_features) # Apply the softmax activation function to the output of the third fully connected layer and return the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the NN has been defined, we can start to initialise it. The input, hidden, and output sizes are defined and then fed into the SimpleNN class to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model, loss function, and optimiser\n",
    "# Input size is the number of input features (e.g., 5 for u, g, r, i, z) we will use\n",
    "input_size = features_train_tensor.shape[1]\n",
    "\n",
    "# Hidden size is the number of neurons in the hidden layer\n",
    "hidden_size = 64  # You can change this value to see how it affects the performance of the model\n",
    "\n",
    "# Output size is the number of classes (e.g., 3 for star, galaxy, quasar)\n",
    "    # The number of classes is the number of unique labels in the training data\n",
    "    # The np.unique() function returns the unique elements in an array, in this case the unique labels in the training data\n",
    "output_size = len(np.unique(label_train_tensor))\n",
    "\n",
    "# Create an instance of the SDSSClassifier class\n",
    "model = SDSSClassifier(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the loss function and optimiser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the loss function and optimiser.\n",
    "\n",
    "- Loss function:\n",
    "    - Used to calculate the error between the predicted output of the neural network and the actual labels\n",
    "    - A hyperparameter that needs to be tuned to achieve the best performance of the model\n",
    "\n",
    "- Optmiser:\n",
    "    - Used to update the weights of the neural network based on the error calculated by the loss function\n",
    "    - There are many different optimisers available in PyTorch, such as Adam, SGD, RMSprop, etc. We use Adam in this example because it is a popular choice for many tasks, and commonly used for NNs\n",
    "    - The Adam optimiser is an adaptive learning rate optimiser that adjusts the learning rate during training, which can help the model converge faster and achieve better performance\n",
    "\n",
    "The loss function and optimiser are defined outside the neural network model class because they are not part of the neural network architecture but are instead used to train the neural network. This also allows them to be easily changed or modified without affecting the neural network architecture.\n",
    "\n",
    "- Learning rate:\n",
    "    - The learning rate controls how much the weights of the neural network are updated during training\n",
    "    - A higher learning rate means the weights are updated more and a smaller learning rate means the weights are updated less, during the training.\n",
    "    - If the learning rate is too high, the model may converge too quickly or diverge, as it overshoots the minimum of the loss function\n",
    "    - If the learning rate is too low, the training process can get stuck in a local minimum and/or take a very long time to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "    # nn.CrossEntropyLoss() is the loss function used for classification tasks with multiple classes\n",
    "    # The CrossEntropyLoss() function combines the softmax activation function and the negative log likelihood loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimiser\n",
    "    # optim.Adam() is the optimiser used to update the weights of the neural network\n",
    "    # The Adam optimiser is an extension of the stochastic gradient descent optimiser\n",
    "    # The Adam optimiser adapts the learning rate for each parameter during training\n",
    "    # The learning rate is specified by the lr argument, which is set to 0.001 (can be finetuned)\n",
    "    # The model.parameters() function specifies the parameters that need to be updated by the optimiser, which in this case are the weights of the neural network\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 0.6059, Val Loss: 0.6102\n",
      "Epoch [2/20], Train Loss: 0.6049, Val Loss: 0.6011\n",
      "Epoch [3/20], Train Loss: 0.6035, Val Loss: 0.6041\n",
      "Epoch [4/20], Train Loss: 0.6031, Val Loss: 0.6010\n",
      "Epoch [5/20], Train Loss: 0.6031, Val Loss: 0.5991\n",
      "Epoch [6/20], Train Loss: 0.6011, Val Loss: 0.6009\n",
      "Epoch [7/20], Train Loss: 0.6007, Val Loss: 0.6003\n",
      "Epoch [8/20], Train Loss: 0.5994, Val Loss: 0.6063\n",
      "Epoch [9/20], Train Loss: 0.5991, Val Loss: 0.5953\n",
      "Epoch [10/20], Train Loss: 0.6006, Val Loss: 0.6047\n",
      "Epoch [11/20], Train Loss: 0.5997, Val Loss: 0.6005\n",
      "Epoch [12/20], Train Loss: 0.5976, Val Loss: 0.5973\n",
      "Epoch [13/20], Train Loss: 0.5962, Val Loss: 0.5957\n",
      "Epoch [14/20], Train Loss: 0.5955, Val Loss: 0.5938\n",
      "Epoch [15/20], Train Loss: 0.5961, Val Loss: 0.5959\n",
      "Epoch [16/20], Train Loss: 0.5960, Val Loss: 0.5936\n",
      "Epoch [17/20], Train Loss: 0.5957, Val Loss: 0.5951\n",
      "Epoch [18/20], Train Loss: 0.5947, Val Loss: 0.5974\n",
      "Epoch [19/20], Train Loss: 0.5955, Val Loss: 0.5956\n",
      "Epoch [20/20], Train Loss: 0.5963, Val Loss: 0.5897\n"
     ]
    }
   ],
   "source": [
    "# Set number of epochs for training\n",
    "num_epochs = 20\n",
    "\n",
    "# Initialize lists to store training and validation loss history\n",
    "train_loss_history = []\n",
    "val_loss_history = []\n",
    "\n",
    "# Train the model for the specified number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Enable training mode for the model (some layers may have different behaviour during training and evaluation)\n",
    "    model.train()\n",
    "\n",
    "    # Set running loss to 0 for each epoch\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the gradient buffers\n",
    "            # It's necessary to zero the gradient buffers before the backward pass to prevent gradient accumulation\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "            # The inputs (features) are passed through the model to obtain the outputs\n",
    "            # The outputs are the class probabilities for each sample\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "            # The loss is computed by comparing the outputs (class probabilities) to the labels (true classes)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass (compute gradients)\n",
    "            # The backward() function computes the gradients of the loss with respect to the model parameters\n",
    "            # The gradients are used to update the weights of the model\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update model parameters\n",
    "            # The step() function updates the weights of the model using the gradients computed in the backward pass\n",
    "            # The optimizer uses the gradients to update the weights according to the optimisation algorithm (Adam in this case)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "            # The loss.item() function returns the loss as a scalar value\n",
    "            # The loss is accumulated for each batch to calculate the average loss for the epoch\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "        # The average loss is calculated by dividing the accumulated loss by the number of batches\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_loss_history.append(train_loss)\n",
    "\n",
    "    # Enable evaluation mode for the model\n",
    "    model.eval()\n",
    "\n",
    "    # Set validation loss to 0 for each epoch\n",
    "    val_loss = 0.0\n",
    "\n",
    "    # Disable gradient calculation during validation\n",
    "        # It's not necessary to calculate gradients during validation\n",
    "        # The no_grad() context manager is used to disable gradient calculation\n",
    "        # Disabling gradient calculation reduces memory consumption and speeds up the computations\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the validation data\n",
    "        for inputs, labels in val_loader:\n",
    "            # Forward pass\n",
    "                # The inputs (features) are passed through the model to obtain the outputs\n",
    "                # The outputs are the class probabilities for each sample\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "                # The loss is computed by comparing the outputs (class probabilities) to the labels (true classes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Accumulate loss\n",
    "                # The loss.item() function returns the loss as a scalar value\n",
    "                # The loss is accumulated for each batch to calculate the average loss for the epoch\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "        # The average loss is calculated by dividing the accumulated loss by the number of batches\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_history.append(val_loss)\n",
    "\n",
    "    # Print training and validation loss for the epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
